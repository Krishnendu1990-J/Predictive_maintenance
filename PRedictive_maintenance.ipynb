{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PRedictive_maintenance.ipynb","provenance":[],"mount_file_id":"1-1jLoNefmJnvjJ1dzd0offDb_mGvfSQw","authorship_tag":"ABX9TyOOFmWMXSyQd/MxTeMY0uMi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"8HgDPUgJ9pSN","executionInfo":{"status":"ok","timestamp":1651632399320,"user_tz":-330,"elapsed":53,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","#import cnn_lstm.utils as util\n","import os"]},{"cell_type":"code","source":["pip install tensorflow==1.14.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLed7Xnbj95G","executionInfo":{"status":"ok","timestamp":1651632453418,"user_tz":-330,"elapsed":46952,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}},"outputId":"9c0f9283-cff3-4753-c13e-b7aaf72ebe0b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==1.14.0\n","  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n","\u001b[K     |████████████████████████████████| 109.3 MB 42 kB/s \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.44.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Collecting tensorboard<1.15.0,>=1.14.0\n","  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 28.8 MB/s \n","\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n","\u001b[K     |████████████████████████████████| 488 kB 47.8 MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.37.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.21.6)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.5.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.1.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0) (1.5.2)\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.0\n","    Uninstalling tensorflow-2.8.0:\n","      Successfully uninstalled tensorflow-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n","Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"]}]},{"cell_type":"code","source":["gap_time = 10  # gap time between each segment\n","win_size = [10, 30, 60]  # window size of each segment\n","step_max = 5 # maximum step of ConvLSTM\n","train_start_id = 10\n","train_end_id = 800\n","\n","test_start_id = 800\n","test_end_id = 2000\n","\n","\n","raw_data_path = '/content/drive/MyDrive/Predictive MAINTENANCE/synthetic_data_with_anomaly-s-1.csv'  # path to load raw data"],"metadata":{"id":"arQXHDyED7pA","executionInfo":{"status":"ok","timestamp":1651632484910,"user_tz":-330,"elapsed":412,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(raw_data_path)"],"metadata":{"id":"0yAzQIn398mg","executionInfo":{"status":"ok","timestamp":1651632517556,"user_tz":-330,"elapsed":2519,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"y0OHx6Bs-Dnr","executionInfo":{"status":"ok","timestamp":1651632540179,"user_tz":-330,"elapsed":441,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}},"outputId":"a1153084-e68a-4b9e-95b2-481d05e07866"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    -1.196068000000000131e+00  -1.158986000000000072e+00  \\\n","0                   -1.409262                  -1.022558   \n","1                   -1.324885                  -0.937945   \n","2                    0.742491                   0.367879   \n","3                    0.623979                   0.468546   \n","4                   -0.963673                  -1.126841   \n","5                   -0.698226                   0.087973   \n","6                   -0.508070                   0.246051   \n","7                   -0.525562                   0.006991   \n","8                   -0.858844                  -0.645605   \n","9                   -1.283104                  -1.369252   \n","10                  -1.241669                  -0.622627   \n","11                   0.509777                   0.278355   \n","12                  -0.738714                  -1.209138   \n","13                  -1.470761                  -1.078936   \n","14                  -1.493144                  -1.063240   \n","15                  -0.054287                   0.189469   \n","16                  -0.183616                  -0.245344   \n","17                  -0.693398                  -0.605014   \n","18                  -0.190042                  -1.015840   \n","19                   0.891600                   0.061827   \n","20                  -0.566915                  -0.353394   \n","21                  -0.805525                  -0.473246   \n","22                   0.167363                   0.563968   \n","23                  -1.113082                  -0.708507   \n","24                  -0.077650                  -0.125898   \n","25                  -1.134439                  -0.494751   \n","26                  -0.364159                  -0.023967   \n","27                   0.107621                   0.779559   \n","28                  -0.875841                  -0.424190   \n","\n","    -8.139640000000000208e-01  -7.668699999999999406e-01  \\\n","0                   -0.661714                  -1.225627   \n","1                   -0.984585                  -1.278234   \n","2                    0.185113                   0.220149   \n","3                    0.773720                   0.425162   \n","4                   -1.174981                  -1.385991   \n","5                   -0.672968                   0.149464   \n","6                    0.410024                  -0.166781   \n","7                    0.514297                  -0.127469   \n","8                   -0.151680                  -0.572383   \n","9                   -1.250062                  -0.895969   \n","10                  -1.162868                  -0.947962   \n","11                  -0.034936                  -0.092271   \n","12                  -0.820720                  -0.635390   \n","13                  -1.057781                  -1.446965   \n","14                  -0.867468                  -1.042330   \n","15                   0.419064                   0.028092   \n","16                   0.178262                  -0.217470   \n","17                  -0.527403                  -0.694237   \n","18                  -0.634866                  -0.825364   \n","19                   0.577232                   0.959456   \n","20                   0.096032                  -0.233595   \n","21                  -0.910825                  -0.897635   \n","22                   0.121327                   0.127617   \n","23                  -1.265556                  -0.237752   \n","24                   0.576044                  -0.188888   \n","25                  -0.924768                  -0.284043   \n","26                  -0.059908                  -0.506532   \n","27                   0.392077                   0.718785   \n","28                  -0.695825                  -0.815191   \n","\n","    -8.482589999999999852e-01  -6.167150000000000132e-01  \\\n","0                   -1.248334                  -0.877054   \n","1                   -0.909862                  -1.540562   \n","2                   -0.155487                   0.299826   \n","3                    0.390874                   0.495360   \n","4                   -1.052355                  -1.133624   \n","5                   -0.505630                   0.156250   \n","6                   -0.481348                   0.219784   \n","7                   -0.227430                   0.085790   \n","8                   -0.419448                  -0.401363   \n","9                   -0.688521                  -0.698091   \n","10                  -1.054963                  -0.771554   \n","11                  -0.305956                  -0.317962   \n","12                  -1.105529                  -1.205741   \n","13                  -0.787215                  -1.164459   \n","14                  -1.066414                  -0.737053   \n","15                   0.507440                  -0.497291   \n","16                   0.278268                   0.365902   \n","17                  -0.838262                  -0.794913   \n","18                  -1.374032                  -0.820366   \n","19                   0.186288                   0.241632   \n","20                   0.282218                  -0.214253   \n","21                  -0.996419                  -0.723873   \n","22                  -0.113775                   0.476507   \n","23                  -1.330478                  -0.989116   \n","24                  -0.019859                   0.091234   \n","25                  -0.808773                  -0.950184   \n","26                  -0.025195                  -0.582588   \n","27                   0.502410                   0.186529   \n","28                  -0.522151                  -1.061208   \n","\n","    -9.835969999999999436e-01  -1.234272999999999954e+00  \\\n","0                   -0.926515                  -0.972395   \n","1                   -1.127760                  -0.995395   \n","2                    1.148501                   0.792409   \n","3                    0.026504                   0.893972   \n","4                   -0.437779                  -0.487339   \n","5                   -0.076724                  -0.087189   \n","6                   -0.164904                  -0.299812   \n","7                    0.437583                   0.177010   \n","8                   -0.534432                  -0.243307   \n","9                   -0.579978                  -0.268245   \n","10                  -0.894815                  -1.019646   \n","11                   0.277928                   0.055682   \n","12                  -1.053453                  -0.962254   \n","13                  -0.667358                  -0.605488   \n","14                  -0.754620                  -0.925710   \n","15                   0.689938                  -0.253194   \n","16                  -0.647074                   0.031247   \n","17                  -1.220642                  -0.970843   \n","18                  -1.781140                  -1.260040   \n","19                   0.530320                   0.386731   \n","20                  -0.601671                  -0.252772   \n","21                  -0.872807                  -0.796361   \n","22                   0.831436                   0.775662   \n","23                  -0.766847                  -0.797820   \n","24                   0.478911                   0.081541   \n","25                  -1.398167                  -1.195881   \n","26                  -0.772059                   0.097613   \n","27                   0.966380                   0.367023   \n","28                  -0.567235                  -0.699764   \n","\n","    -8.933330000000000437e-01  -8.880799999999999805e-01  ...  \\\n","0                   -1.310417                  -0.308838  ...   \n","1                   -0.485886                  -1.029982  ...   \n","2                    0.781007                   0.984796  ...   \n","3                    0.684652                   0.364529  ...   \n","4                   -0.409925                  -1.421387  ...   \n","5                   -0.285001                  -0.424794  ...   \n","6                    0.001180                   0.017570  ...   \n","7                    1.093951                   0.425636  ...   \n","8                    0.106056                  -0.203455  ...   \n","9                   -0.989950                  -0.805572  ...   \n","10                  -0.685825                  -0.225443  ...   \n","11                   0.195697                   0.070832  ...   \n","12                  -1.088394                  -0.576620  ...   \n","13                  -1.101485                  -0.652172  ...   \n","14                  -0.824499                  -0.736181  ...   \n","15                   0.099862                   0.237715  ...   \n","16                   0.645429                   0.867373  ...   \n","17                  -0.838141                  -0.752470  ...   \n","18                  -1.020457                  -1.396890  ...   \n","19                   0.829109                   0.281560  ...   \n","20                  -0.238030                  -0.207126  ...   \n","21                  -0.937298                  -0.998545  ...   \n","22                   1.050560                   0.506162  ...   \n","23                  -0.553989                  -0.821595  ...   \n","24                   0.206716                   0.485552  ...   \n","25                  -0.298126                  -1.321207  ...   \n","26                  -0.017329                   0.673002  ...   \n","27                   0.239763                   1.219137  ...   \n","28                  -0.999267                  -0.368801  ...   \n","\n","    -6.341700000000000115e-01  -6.862580000000000346e-01  \\\n","0                    1.012659                   0.671358   \n","1                   -0.988206                  -0.520310   \n","2                   -1.174283                  -0.620993   \n","3                   -1.238904                  -0.878401   \n","4                    0.640844                   0.444542   \n","5                   -0.189776                  -0.414064   \n","6                   -0.829821                  -1.123251   \n","7                    0.234202                   0.013695   \n","8                    0.531312                   0.573123   \n","9                   -0.201858                   0.071232   \n","10                   0.037747                  -0.036915   \n","11                  -0.610300                  -0.565181   \n","12                   1.066213                   1.076884   \n","13                   0.891497                   0.042738   \n","14                  -0.171080                  -0.113057   \n","15                  -0.541069                   0.380645   \n","16                  -0.647550                  -0.253515   \n","17                   0.807834                   0.357554   \n","18                   0.005680                   0.800555   \n","19                   0.910089                   1.145159   \n","20                   0.103446                   0.504975   \n","21                   0.515575                  -0.197192   \n","22                  -0.710445                  -0.983012   \n","23                   0.430764                  -0.356877   \n","24                  -0.150396                   0.611134   \n","25                  -0.816613                  -0.893666   \n","26                   0.059615                   0.515416   \n","27                  -0.757785                  -0.888403   \n","28                   0.366114                   0.442638   \n","\n","    -1.332629000000000064e+00  -1.363879999999999981e+00  \\\n","0                    0.260170                   1.084126   \n","1                   -0.505600                  -0.356896   \n","2                   -0.810129                  -0.468340   \n","3                   -0.781293                  -0.990054   \n","4                    0.324334                   0.156289   \n","5                   -1.069757                  -0.509477   \n","6                   -0.642230                  -0.750705   \n","7                    0.043447                  -0.311086   \n","8                    0.341785                   0.450970   \n","9                   -0.305412                   0.022543   \n","10                   0.219227                  -0.125695   \n","11                  -1.132153                  -0.516969   \n","12                   0.651212                   0.669011   \n","13                  -0.023564                   0.934238   \n","14                  -0.433039                  -0.690613   \n","15                  -0.396244                  -0.160650   \n","16                  -0.952984                  -1.105763   \n","17                   0.563749                   0.793824   \n","18                   0.244588                   0.898102   \n","19                   0.940515                   1.158793   \n","20                   0.107001                   0.118837   \n","21                  -0.026254                  -0.375029   \n","22                  -1.210775                  -1.084826   \n","23                  -0.284991                  -0.030557   \n","24                   0.172017                   0.479004   \n","25                  -0.524585                  -1.244381   \n","26                   0.184954                  -0.399899   \n","27                  -0.978317                  -0.458760   \n","28                  -0.293206                   0.021817   \n","\n","    -8.774129999999998875e-01  -1.108570000000000055e+00  \\\n","0                    0.421436                   0.356639   \n","1                   -0.116222                  -0.148276   \n","2                   -0.666751                  -0.769575   \n","3                   -1.309617                  -0.697161   \n","4                    0.155061                   0.187091   \n","5                   -0.099970                  -0.653943   \n","6                   -1.093439                  -1.102578   \n","7                   -0.120940                   0.055676   \n","8                    0.396964                   0.292883   \n","9                   -0.477208                  -0.321301   \n","10                  -0.084795                   0.016139   \n","11                  -0.897525                  -1.442245   \n","12                   0.561115                   1.229990   \n","13                   0.327242                   0.401078   \n","14                  -0.908442                  -0.318215   \n","15                  -0.093855                  -0.811688   \n","16                  -0.909042                  -0.974958   \n","17                  -0.337225                   0.118502   \n","18                   0.443014                   0.817273   \n","19                   1.105878                   0.003735   \n","20                  -0.272205                   0.121466   \n","21                  -0.733429                  -0.224914   \n","22                  -0.594701                  -1.092743   \n","23                  -0.569973                   0.033067   \n","24                   0.315285                   0.643732   \n","25                  -1.043159                  -1.031581   \n","26                   0.048292                  -0.436434   \n","27                  -0.975863                  -1.024026   \n","28                  -0.254014                  -0.155868   \n","\n","    -6.994540000000000202e-01  -3.833849999999999758e-01  \\\n","0                   -0.033012                   0.434981   \n","1                   -0.007345                  -0.521726   \n","2                   -0.879158                  -1.089526   \n","3                   -0.874367                  -1.164499   \n","4                    0.238768                   0.617186   \n","5                    0.066006                  -0.499934   \n","6                   -0.856798                  -1.175703   \n","7                    0.015557                  -0.006210   \n","8                    0.607741                   0.747780   \n","9                   -0.142132                  -0.641109   \n","10                  -0.697203                   0.401185   \n","11                  -0.977352                  -1.522188   \n","12                   1.152450                   1.168824   \n","13                   0.573351                   0.505429   \n","14                  -0.835325                  -1.066208   \n","15                  -0.898476                   0.072263   \n","16                  -0.623336                  -0.636232   \n","17                   0.075763                   0.236755   \n","18                   0.409367                   0.279017   \n","19                   0.769871                   1.033247   \n","20                  -0.488817                  -0.384215   \n","21                   0.144228                   0.225298   \n","22                  -1.145491                  -0.973316   \n","23                  -0.005860                  -0.879997   \n","24                   0.403434                   0.583017   \n","25                  -0.778862                  -0.968665   \n","26                  -0.422959                  -0.233750   \n","27                  -0.862832                  -0.424515   \n","28                   0.374806                  -0.096655   \n","\n","    -7.070680000000000298e-01  -6.410630000000000495e-01  \n","0                   -0.142920                  -0.240561  \n","1                   -0.731946                  -0.362072  \n","2                   -0.968375                  -0.592400  \n","3                   -1.157023                  -0.807515  \n","4                    0.198136                   0.677872  \n","5                   -0.274588                  -0.823938  \n","6                   -0.429905                  -0.565261  \n","7                    0.450147                  -0.676634  \n","8                    0.570295                   0.600242  \n","9                   -0.073734                   0.391355  \n","10                   0.308003                  -0.217310  \n","11                   0.104807                  -0.618989  \n","12                   0.977371                   1.335423  \n","13                   0.400966                   0.587954  \n","14                  -0.629061                  -1.440360  \n","15                  -0.715908                  -0.311527  \n","16                  -0.636681                  -0.719089  \n","17                   0.109628                  -0.286730  \n","18                   0.265802                   0.024785  \n","19                   0.643450                   0.427920  \n","20                  -0.095476                   0.328822  \n","21                  -0.210370                   0.378875  \n","22                  -1.185389                  -0.690139  \n","23                  -0.289232                  -0.218868  \n","24                   0.198923                   0.446848  \n","25                  -0.923387                  -1.051300  \n","26                  -0.832166                  -0.180481  \n","27                  -1.098415                  -0.873634  \n","28                  -0.626115                  -0.024076  \n","\n","[29 rows x 20000 columns]"],"text/html":["\n","  <div id=\"df-ab648c2b-9090-4146-af8d-5f2f6a3491fe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>-1.196068000000000131e+00</th>\n","      <th>-1.158986000000000072e+00</th>\n","      <th>-8.139640000000000208e-01</th>\n","      <th>-7.668699999999999406e-01</th>\n","      <th>-8.482589999999999852e-01</th>\n","      <th>-6.167150000000000132e-01</th>\n","      <th>-9.835969999999999436e-01</th>\n","      <th>-1.234272999999999954e+00</th>\n","      <th>-8.933330000000000437e-01</th>\n","      <th>-8.880799999999999805e-01</th>\n","      <th>...</th>\n","      <th>-6.341700000000000115e-01</th>\n","      <th>-6.862580000000000346e-01</th>\n","      <th>-1.332629000000000064e+00</th>\n","      <th>-1.363879999999999981e+00</th>\n","      <th>-8.774129999999998875e-01</th>\n","      <th>-1.108570000000000055e+00</th>\n","      <th>-6.994540000000000202e-01</th>\n","      <th>-3.833849999999999758e-01</th>\n","      <th>-7.070680000000000298e-01</th>\n","      <th>-6.410630000000000495e-01</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.409262</td>\n","      <td>-1.022558</td>\n","      <td>-0.661714</td>\n","      <td>-1.225627</td>\n","      <td>-1.248334</td>\n","      <td>-0.877054</td>\n","      <td>-0.926515</td>\n","      <td>-0.972395</td>\n","      <td>-1.310417</td>\n","      <td>-0.308838</td>\n","      <td>...</td>\n","      <td>1.012659</td>\n","      <td>0.671358</td>\n","      <td>0.260170</td>\n","      <td>1.084126</td>\n","      <td>0.421436</td>\n","      <td>0.356639</td>\n","      <td>-0.033012</td>\n","      <td>0.434981</td>\n","      <td>-0.142920</td>\n","      <td>-0.240561</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1.324885</td>\n","      <td>-0.937945</td>\n","      <td>-0.984585</td>\n","      <td>-1.278234</td>\n","      <td>-0.909862</td>\n","      <td>-1.540562</td>\n","      <td>-1.127760</td>\n","      <td>-0.995395</td>\n","      <td>-0.485886</td>\n","      <td>-1.029982</td>\n","      <td>...</td>\n","      <td>-0.988206</td>\n","      <td>-0.520310</td>\n","      <td>-0.505600</td>\n","      <td>-0.356896</td>\n","      <td>-0.116222</td>\n","      <td>-0.148276</td>\n","      <td>-0.007345</td>\n","      <td>-0.521726</td>\n","      <td>-0.731946</td>\n","      <td>-0.362072</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.742491</td>\n","      <td>0.367879</td>\n","      <td>0.185113</td>\n","      <td>0.220149</td>\n","      <td>-0.155487</td>\n","      <td>0.299826</td>\n","      <td>1.148501</td>\n","      <td>0.792409</td>\n","      <td>0.781007</td>\n","      <td>0.984796</td>\n","      <td>...</td>\n","      <td>-1.174283</td>\n","      <td>-0.620993</td>\n","      <td>-0.810129</td>\n","      <td>-0.468340</td>\n","      <td>-0.666751</td>\n","      <td>-0.769575</td>\n","      <td>-0.879158</td>\n","      <td>-1.089526</td>\n","      <td>-0.968375</td>\n","      <td>-0.592400</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.623979</td>\n","      <td>0.468546</td>\n","      <td>0.773720</td>\n","      <td>0.425162</td>\n","      <td>0.390874</td>\n","      <td>0.495360</td>\n","      <td>0.026504</td>\n","      <td>0.893972</td>\n","      <td>0.684652</td>\n","      <td>0.364529</td>\n","      <td>...</td>\n","      <td>-1.238904</td>\n","      <td>-0.878401</td>\n","      <td>-0.781293</td>\n","      <td>-0.990054</td>\n","      <td>-1.309617</td>\n","      <td>-0.697161</td>\n","      <td>-0.874367</td>\n","      <td>-1.164499</td>\n","      <td>-1.157023</td>\n","      <td>-0.807515</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.963673</td>\n","      <td>-1.126841</td>\n","      <td>-1.174981</td>\n","      <td>-1.385991</td>\n","      <td>-1.052355</td>\n","      <td>-1.133624</td>\n","      <td>-0.437779</td>\n","      <td>-0.487339</td>\n","      <td>-0.409925</td>\n","      <td>-1.421387</td>\n","      <td>...</td>\n","      <td>0.640844</td>\n","      <td>0.444542</td>\n","      <td>0.324334</td>\n","      <td>0.156289</td>\n","      <td>0.155061</td>\n","      <td>0.187091</td>\n","      <td>0.238768</td>\n","      <td>0.617186</td>\n","      <td>0.198136</td>\n","      <td>0.677872</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>-0.698226</td>\n","      <td>0.087973</td>\n","      <td>-0.672968</td>\n","      <td>0.149464</td>\n","      <td>-0.505630</td>\n","      <td>0.156250</td>\n","      <td>-0.076724</td>\n","      <td>-0.087189</td>\n","      <td>-0.285001</td>\n","      <td>-0.424794</td>\n","      <td>...</td>\n","      <td>-0.189776</td>\n","      <td>-0.414064</td>\n","      <td>-1.069757</td>\n","      <td>-0.509477</td>\n","      <td>-0.099970</td>\n","      <td>-0.653943</td>\n","      <td>0.066006</td>\n","      <td>-0.499934</td>\n","      <td>-0.274588</td>\n","      <td>-0.823938</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>-0.508070</td>\n","      <td>0.246051</td>\n","      <td>0.410024</td>\n","      <td>-0.166781</td>\n","      <td>-0.481348</td>\n","      <td>0.219784</td>\n","      <td>-0.164904</td>\n","      <td>-0.299812</td>\n","      <td>0.001180</td>\n","      <td>0.017570</td>\n","      <td>...</td>\n","      <td>-0.829821</td>\n","      <td>-1.123251</td>\n","      <td>-0.642230</td>\n","      <td>-0.750705</td>\n","      <td>-1.093439</td>\n","      <td>-1.102578</td>\n","      <td>-0.856798</td>\n","      <td>-1.175703</td>\n","      <td>-0.429905</td>\n","      <td>-0.565261</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-0.525562</td>\n","      <td>0.006991</td>\n","      <td>0.514297</td>\n","      <td>-0.127469</td>\n","      <td>-0.227430</td>\n","      <td>0.085790</td>\n","      <td>0.437583</td>\n","      <td>0.177010</td>\n","      <td>1.093951</td>\n","      <td>0.425636</td>\n","      <td>...</td>\n","      <td>0.234202</td>\n","      <td>0.013695</td>\n","      <td>0.043447</td>\n","      <td>-0.311086</td>\n","      <td>-0.120940</td>\n","      <td>0.055676</td>\n","      <td>0.015557</td>\n","      <td>-0.006210</td>\n","      <td>0.450147</td>\n","      <td>-0.676634</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>-0.858844</td>\n","      <td>-0.645605</td>\n","      <td>-0.151680</td>\n","      <td>-0.572383</td>\n","      <td>-0.419448</td>\n","      <td>-0.401363</td>\n","      <td>-0.534432</td>\n","      <td>-0.243307</td>\n","      <td>0.106056</td>\n","      <td>-0.203455</td>\n","      <td>...</td>\n","      <td>0.531312</td>\n","      <td>0.573123</td>\n","      <td>0.341785</td>\n","      <td>0.450970</td>\n","      <td>0.396964</td>\n","      <td>0.292883</td>\n","      <td>0.607741</td>\n","      <td>0.747780</td>\n","      <td>0.570295</td>\n","      <td>0.600242</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-1.283104</td>\n","      <td>-1.369252</td>\n","      <td>-1.250062</td>\n","      <td>-0.895969</td>\n","      <td>-0.688521</td>\n","      <td>-0.698091</td>\n","      <td>-0.579978</td>\n","      <td>-0.268245</td>\n","      <td>-0.989950</td>\n","      <td>-0.805572</td>\n","      <td>...</td>\n","      <td>-0.201858</td>\n","      <td>0.071232</td>\n","      <td>-0.305412</td>\n","      <td>0.022543</td>\n","      <td>-0.477208</td>\n","      <td>-0.321301</td>\n","      <td>-0.142132</td>\n","      <td>-0.641109</td>\n","      <td>-0.073734</td>\n","      <td>0.391355</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>-1.241669</td>\n","      <td>-0.622627</td>\n","      <td>-1.162868</td>\n","      <td>-0.947962</td>\n","      <td>-1.054963</td>\n","      <td>-0.771554</td>\n","      <td>-0.894815</td>\n","      <td>-1.019646</td>\n","      <td>-0.685825</td>\n","      <td>-0.225443</td>\n","      <td>...</td>\n","      <td>0.037747</td>\n","      <td>-0.036915</td>\n","      <td>0.219227</td>\n","      <td>-0.125695</td>\n","      <td>-0.084795</td>\n","      <td>0.016139</td>\n","      <td>-0.697203</td>\n","      <td>0.401185</td>\n","      <td>0.308003</td>\n","      <td>-0.217310</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.509777</td>\n","      <td>0.278355</td>\n","      <td>-0.034936</td>\n","      <td>-0.092271</td>\n","      <td>-0.305956</td>\n","      <td>-0.317962</td>\n","      <td>0.277928</td>\n","      <td>0.055682</td>\n","      <td>0.195697</td>\n","      <td>0.070832</td>\n","      <td>...</td>\n","      <td>-0.610300</td>\n","      <td>-0.565181</td>\n","      <td>-1.132153</td>\n","      <td>-0.516969</td>\n","      <td>-0.897525</td>\n","      <td>-1.442245</td>\n","      <td>-0.977352</td>\n","      <td>-1.522188</td>\n","      <td>0.104807</td>\n","      <td>-0.618989</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>-0.738714</td>\n","      <td>-1.209138</td>\n","      <td>-0.820720</td>\n","      <td>-0.635390</td>\n","      <td>-1.105529</td>\n","      <td>-1.205741</td>\n","      <td>-1.053453</td>\n","      <td>-0.962254</td>\n","      <td>-1.088394</td>\n","      <td>-0.576620</td>\n","      <td>...</td>\n","      <td>1.066213</td>\n","      <td>1.076884</td>\n","      <td>0.651212</td>\n","      <td>0.669011</td>\n","      <td>0.561115</td>\n","      <td>1.229990</td>\n","      <td>1.152450</td>\n","      <td>1.168824</td>\n","      <td>0.977371</td>\n","      <td>1.335423</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>-1.470761</td>\n","      <td>-1.078936</td>\n","      <td>-1.057781</td>\n","      <td>-1.446965</td>\n","      <td>-0.787215</td>\n","      <td>-1.164459</td>\n","      <td>-0.667358</td>\n","      <td>-0.605488</td>\n","      <td>-1.101485</td>\n","      <td>-0.652172</td>\n","      <td>...</td>\n","      <td>0.891497</td>\n","      <td>0.042738</td>\n","      <td>-0.023564</td>\n","      <td>0.934238</td>\n","      <td>0.327242</td>\n","      <td>0.401078</td>\n","      <td>0.573351</td>\n","      <td>0.505429</td>\n","      <td>0.400966</td>\n","      <td>0.587954</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>-1.493144</td>\n","      <td>-1.063240</td>\n","      <td>-0.867468</td>\n","      <td>-1.042330</td>\n","      <td>-1.066414</td>\n","      <td>-0.737053</td>\n","      <td>-0.754620</td>\n","      <td>-0.925710</td>\n","      <td>-0.824499</td>\n","      <td>-0.736181</td>\n","      <td>...</td>\n","      <td>-0.171080</td>\n","      <td>-0.113057</td>\n","      <td>-0.433039</td>\n","      <td>-0.690613</td>\n","      <td>-0.908442</td>\n","      <td>-0.318215</td>\n","      <td>-0.835325</td>\n","      <td>-1.066208</td>\n","      <td>-0.629061</td>\n","      <td>-1.440360</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>-0.054287</td>\n","      <td>0.189469</td>\n","      <td>0.419064</td>\n","      <td>0.028092</td>\n","      <td>0.507440</td>\n","      <td>-0.497291</td>\n","      <td>0.689938</td>\n","      <td>-0.253194</td>\n","      <td>0.099862</td>\n","      <td>0.237715</td>\n","      <td>...</td>\n","      <td>-0.541069</td>\n","      <td>0.380645</td>\n","      <td>-0.396244</td>\n","      <td>-0.160650</td>\n","      <td>-0.093855</td>\n","      <td>-0.811688</td>\n","      <td>-0.898476</td>\n","      <td>0.072263</td>\n","      <td>-0.715908</td>\n","      <td>-0.311527</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>-0.183616</td>\n","      <td>-0.245344</td>\n","      <td>0.178262</td>\n","      <td>-0.217470</td>\n","      <td>0.278268</td>\n","      <td>0.365902</td>\n","      <td>-0.647074</td>\n","      <td>0.031247</td>\n","      <td>0.645429</td>\n","      <td>0.867373</td>\n","      <td>...</td>\n","      <td>-0.647550</td>\n","      <td>-0.253515</td>\n","      <td>-0.952984</td>\n","      <td>-1.105763</td>\n","      <td>-0.909042</td>\n","      <td>-0.974958</td>\n","      <td>-0.623336</td>\n","      <td>-0.636232</td>\n","      <td>-0.636681</td>\n","      <td>-0.719089</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>-0.693398</td>\n","      <td>-0.605014</td>\n","      <td>-0.527403</td>\n","      <td>-0.694237</td>\n","      <td>-0.838262</td>\n","      <td>-0.794913</td>\n","      <td>-1.220642</td>\n","      <td>-0.970843</td>\n","      <td>-0.838141</td>\n","      <td>-0.752470</td>\n","      <td>...</td>\n","      <td>0.807834</td>\n","      <td>0.357554</td>\n","      <td>0.563749</td>\n","      <td>0.793824</td>\n","      <td>-0.337225</td>\n","      <td>0.118502</td>\n","      <td>0.075763</td>\n","      <td>0.236755</td>\n","      <td>0.109628</td>\n","      <td>-0.286730</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>-0.190042</td>\n","      <td>-1.015840</td>\n","      <td>-0.634866</td>\n","      <td>-0.825364</td>\n","      <td>-1.374032</td>\n","      <td>-0.820366</td>\n","      <td>-1.781140</td>\n","      <td>-1.260040</td>\n","      <td>-1.020457</td>\n","      <td>-1.396890</td>\n","      <td>...</td>\n","      <td>0.005680</td>\n","      <td>0.800555</td>\n","      <td>0.244588</td>\n","      <td>0.898102</td>\n","      <td>0.443014</td>\n","      <td>0.817273</td>\n","      <td>0.409367</td>\n","      <td>0.279017</td>\n","      <td>0.265802</td>\n","      <td>0.024785</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.891600</td>\n","      <td>0.061827</td>\n","      <td>0.577232</td>\n","      <td>0.959456</td>\n","      <td>0.186288</td>\n","      <td>0.241632</td>\n","      <td>0.530320</td>\n","      <td>0.386731</td>\n","      <td>0.829109</td>\n","      <td>0.281560</td>\n","      <td>...</td>\n","      <td>0.910089</td>\n","      <td>1.145159</td>\n","      <td>0.940515</td>\n","      <td>1.158793</td>\n","      <td>1.105878</td>\n","      <td>0.003735</td>\n","      <td>0.769871</td>\n","      <td>1.033247</td>\n","      <td>0.643450</td>\n","      <td>0.427920</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>-0.566915</td>\n","      <td>-0.353394</td>\n","      <td>0.096032</td>\n","      <td>-0.233595</td>\n","      <td>0.282218</td>\n","      <td>-0.214253</td>\n","      <td>-0.601671</td>\n","      <td>-0.252772</td>\n","      <td>-0.238030</td>\n","      <td>-0.207126</td>\n","      <td>...</td>\n","      <td>0.103446</td>\n","      <td>0.504975</td>\n","      <td>0.107001</td>\n","      <td>0.118837</td>\n","      <td>-0.272205</td>\n","      <td>0.121466</td>\n","      <td>-0.488817</td>\n","      <td>-0.384215</td>\n","      <td>-0.095476</td>\n","      <td>0.328822</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>-0.805525</td>\n","      <td>-0.473246</td>\n","      <td>-0.910825</td>\n","      <td>-0.897635</td>\n","      <td>-0.996419</td>\n","      <td>-0.723873</td>\n","      <td>-0.872807</td>\n","      <td>-0.796361</td>\n","      <td>-0.937298</td>\n","      <td>-0.998545</td>\n","      <td>...</td>\n","      <td>0.515575</td>\n","      <td>-0.197192</td>\n","      <td>-0.026254</td>\n","      <td>-0.375029</td>\n","      <td>-0.733429</td>\n","      <td>-0.224914</td>\n","      <td>0.144228</td>\n","      <td>0.225298</td>\n","      <td>-0.210370</td>\n","      <td>0.378875</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.167363</td>\n","      <td>0.563968</td>\n","      <td>0.121327</td>\n","      <td>0.127617</td>\n","      <td>-0.113775</td>\n","      <td>0.476507</td>\n","      <td>0.831436</td>\n","      <td>0.775662</td>\n","      <td>1.050560</td>\n","      <td>0.506162</td>\n","      <td>...</td>\n","      <td>-0.710445</td>\n","      <td>-0.983012</td>\n","      <td>-1.210775</td>\n","      <td>-1.084826</td>\n","      <td>-0.594701</td>\n","      <td>-1.092743</td>\n","      <td>-1.145491</td>\n","      <td>-0.973316</td>\n","      <td>-1.185389</td>\n","      <td>-0.690139</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>-1.113082</td>\n","      <td>-0.708507</td>\n","      <td>-1.265556</td>\n","      <td>-0.237752</td>\n","      <td>-1.330478</td>\n","      <td>-0.989116</td>\n","      <td>-0.766847</td>\n","      <td>-0.797820</td>\n","      <td>-0.553989</td>\n","      <td>-0.821595</td>\n","      <td>...</td>\n","      <td>0.430764</td>\n","      <td>-0.356877</td>\n","      <td>-0.284991</td>\n","      <td>-0.030557</td>\n","      <td>-0.569973</td>\n","      <td>0.033067</td>\n","      <td>-0.005860</td>\n","      <td>-0.879997</td>\n","      <td>-0.289232</td>\n","      <td>-0.218868</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>-0.077650</td>\n","      <td>-0.125898</td>\n","      <td>0.576044</td>\n","      <td>-0.188888</td>\n","      <td>-0.019859</td>\n","      <td>0.091234</td>\n","      <td>0.478911</td>\n","      <td>0.081541</td>\n","      <td>0.206716</td>\n","      <td>0.485552</td>\n","      <td>...</td>\n","      <td>-0.150396</td>\n","      <td>0.611134</td>\n","      <td>0.172017</td>\n","      <td>0.479004</td>\n","      <td>0.315285</td>\n","      <td>0.643732</td>\n","      <td>0.403434</td>\n","      <td>0.583017</td>\n","      <td>0.198923</td>\n","      <td>0.446848</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>-1.134439</td>\n","      <td>-0.494751</td>\n","      <td>-0.924768</td>\n","      <td>-0.284043</td>\n","      <td>-0.808773</td>\n","      <td>-0.950184</td>\n","      <td>-1.398167</td>\n","      <td>-1.195881</td>\n","      <td>-0.298126</td>\n","      <td>-1.321207</td>\n","      <td>...</td>\n","      <td>-0.816613</td>\n","      <td>-0.893666</td>\n","      <td>-0.524585</td>\n","      <td>-1.244381</td>\n","      <td>-1.043159</td>\n","      <td>-1.031581</td>\n","      <td>-0.778862</td>\n","      <td>-0.968665</td>\n","      <td>-0.923387</td>\n","      <td>-1.051300</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>-0.364159</td>\n","      <td>-0.023967</td>\n","      <td>-0.059908</td>\n","      <td>-0.506532</td>\n","      <td>-0.025195</td>\n","      <td>-0.582588</td>\n","      <td>-0.772059</td>\n","      <td>0.097613</td>\n","      <td>-0.017329</td>\n","      <td>0.673002</td>\n","      <td>...</td>\n","      <td>0.059615</td>\n","      <td>0.515416</td>\n","      <td>0.184954</td>\n","      <td>-0.399899</td>\n","      <td>0.048292</td>\n","      <td>-0.436434</td>\n","      <td>-0.422959</td>\n","      <td>-0.233750</td>\n","      <td>-0.832166</td>\n","      <td>-0.180481</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.107621</td>\n","      <td>0.779559</td>\n","      <td>0.392077</td>\n","      <td>0.718785</td>\n","      <td>0.502410</td>\n","      <td>0.186529</td>\n","      <td>0.966380</td>\n","      <td>0.367023</td>\n","      <td>0.239763</td>\n","      <td>1.219137</td>\n","      <td>...</td>\n","      <td>-0.757785</td>\n","      <td>-0.888403</td>\n","      <td>-0.978317</td>\n","      <td>-0.458760</td>\n","      <td>-0.975863</td>\n","      <td>-1.024026</td>\n","      <td>-0.862832</td>\n","      <td>-0.424515</td>\n","      <td>-1.098415</td>\n","      <td>-0.873634</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>-0.875841</td>\n","      <td>-0.424190</td>\n","      <td>-0.695825</td>\n","      <td>-0.815191</td>\n","      <td>-0.522151</td>\n","      <td>-1.061208</td>\n","      <td>-0.567235</td>\n","      <td>-0.699764</td>\n","      <td>-0.999267</td>\n","      <td>-0.368801</td>\n","      <td>...</td>\n","      <td>0.366114</td>\n","      <td>0.442638</td>\n","      <td>-0.293206</td>\n","      <td>0.021817</td>\n","      <td>-0.254014</td>\n","      <td>-0.155868</td>\n","      <td>0.374806</td>\n","      <td>-0.096655</td>\n","      <td>-0.626115</td>\n","      <td>-0.024076</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29 rows × 20000 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab648c2b-9090-4146-af8d-5f2f6a3491fe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ab648c2b-9090-4146-af8d-5f2f6a3491fe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ab648c2b-9090-4146-af8d-5f2f6a3491fe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[" win= 10\n"," for t in range(win,signature_matrices_number):\n","        raw_data_t = raw_data[:, t - win:t]\n","        print(t-win)\n","        signature_matrices[t] = np.dot(raw_data_t, raw_data_t.T) / win"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"S1mmxqbqAS2r","executionInfo":{"status":"error","timestamp":1651633301702,"user_tz":-330,"elapsed":407,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}},"outputId":"cbfff22d-a485-4b48-b644-75ca343e53e5"},"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d554a50b1119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msignature_matrices_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m        \u001b[0mraw_data_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0msignature_matrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'signature_matrices_number' is not defined"]}]},{"cell_type":"code","source":["class SignatureMatrices:\n","    def __init__(self):\n","\n","        self.raw_data = pd.read_csv(raw_data_path, header=None)\n","        self.series_number = self.raw_data.shape[0]\n","        self.series_length = self.raw_data.shape[1]\n","        self.signature_matrices_number = int(self.series_length /gap_time)\n","\n","        print(\"series_number is\", self.series_number)\n","        print(\"series_length is\", self.series_length)\n","        print(\"signature_matrices_number is\", self.signature_matrices_number)\n","\n","    def signature_matrices_generation(self, win):\n","        \"\"\"\n","        Generation signature matrices according win_size and gap_time, the size of raw_data is n * T, n is the number of\n","        time series, T is the length of time series.\n","        To represent the inter-correlations between different pairs of time series in a multivariate time series segment\n","        from t − w to t, we construct an n × n signature matrix Mt based upon the pairwise inner-product of two time series\n","        within this segment.\n","        :param win: the length of the time series segment\n","        :return: the signature matrices\n","        \"\"\"\n","\n","        if win == 0:\n","            print(\"The size of win cannot be 0\")\n","\n","        raw_data = np.asarray(self.raw_data)\n","        signature_matrices = np.zeros((self.signature_matrices_number, self.series_number, self.series_number))\n","\n","        for t in range(win, self.signature_matrices_number):\n","            raw_data_t = raw_data[:, t - win:t]\n","            print(raw_data_t)\n","            signature_matrices[t] = np.dot(raw_data_t, raw_data_t.T) / win\n","\n","        return signature_matrices\n","\n","    def generate_train_test(self, signature_matrices):\n","        \"\"\"\n","        Generate train and test dataset, and store them to ../data/train/train.npy and ../data/test/test.npy\n","        :param signature_matrices:\n","        :return:\n","        \"\"\"\n","        train_dataset = []\n","        test_dataset = []\n","\n","        for data_id in range(self.signature_matrices_number):\n","            index = data_id - step_max + 1\n","            if data_id < train_start_id:\n","                continue\n","            index_dataset = signature_matrices[:, index:index + step_max]\n","            if data_id < test_start_id:\n","                train_dataset.append(index_dataset)\n","            else:\n","                test_dataset.append(index_dataset)\n","\n","        train_dataset = np.asarray(train_dataset)\n","        train_dataset = np.reshape(train_dataset, [-1, step_max, self.series_number, self.series_number,\n","                                                   signature_matrices.shape[0]])\n","        test_dataset = np.asarray(test_dataset)\n","        test_dataset = np.reshape(test_dataset, [-1, step_max,self.series_number, self.series_number,\n","                                                signature_matrices.shape[0]])\n","\n","        print(\"train dataset shape is\", train_dataset.shape)\n","        print(\"test dataset shape is\", test_dataset.shape)\n","\n","        train_path = \"/content/drive/MyDrive/Predictive MAINTENANCE/train\"\n","        if not os.path.exists(train_path):\n","            os.makedirs(train_path)\n","        train_path = train_path + \"train.npy\"\n","\n","        test_path= \"/content/drive/MyDrive/Predictive MAINTENANCE/test\"\n","        if not os.path.exists(test_path):\n","            os.makedirs(test_path)\n","        test_path = test_path + \"test.npy\"\n","\n","        np.save(train_path, train_dataset)\n","        np.save(test_path, test_dataset)\n","\n","\n","if __name__ == '__main__':\n","    Matrices = SignatureMatrices()\n","    signature_matrices = []\n","\n","    # Generation signature matrices according the win size w\n","    for w in win_size:\n","        signature_matrices.append(Matrices.signature_matrices_generation(w))\n","\n","    signature_matrices = np.asarray(signature_matrices)\n","    print(\"the shape of signature_matrices is\", signature_matrices.shape)\n","\n","    # Generate train and test dataset\n","    Matrices.generate_train_test(signature_matrices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1ajcb0UVRQHt5bqhXVOgPSXQeO8TYPP4d"},"id":"F7HCwrUJDZiT","executionInfo":{"status":"ok","timestamp":1651632936529,"user_tz":-330,"elapsed":35222,"user":{"displayName":"Krishnendu Janardhanan","userId":"02942996957088729370"}},"outputId":"44c7d7c4-5879-4804-d342-7fd6fabe0950"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"40LbWsYObcZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_encoder_layer(data, filter_layer, strides):\n","    \"\"\"\n","    :param data: the input data, when it is the first layer is 5 * 30 * 30 * 3, the second layer is 30 * 30 * 32,\n","                 the third layer is 15 * 15 * 64, the fourth layer is 8 * 8 * 128\n","    :param filter_layer:\n","    :param strides:\n","    :return: the result after conv, the first layer is 30 * 30 * 32, the second layer is 15 * 15 * 64, the third layer\n","             is 8 * 8 * 128, the final layer is 4 * 4 * 256\n","    \"\"\"\n","\n","    result = tf.nn.conv2d(\n","        input=data,\n","        filter=filter_layer,\n","        strides=strides,\n","        padding=\"SAME\")\n","    return tf.nn.selu(result)\n","\n","\n","def tensor_variable(shape, name):\n","    \"\"\"\n","    Tensor variable declaration initialization\n","    :param shape:\n","    :param name:\n","    :return:\n","    \"\"\"\n","    variable = tf.Variable(tf.zeros(shape), name=name)\n","    variable = tf.compat.v1.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n","    return variable\n","\n","\n","def cnn_encoder(data):\n","    \"\"\"\n","    :param data: the input data size is 5 * 30 * 30 * 3\n","    :return:\n","    \"\"\"\n","    # the first layer,the output size is 30 * 30 * 32\n","    filter1 = tensor_variable([3, 3, 3, 32], \"filter1\")\n","    strides1 = (1, 1, 1, 1)\n","    cnn1_out = cnn_encoder_layer(data, filter1, strides1)\n","\n","    # the second layer, the output size is 15 * 15 * 64\n","    filter2 = tensor_variable([3, 3, 32, 64], \"filter2\")\n","    strides2 = (1, 2, 2, 1)\n","    cnn2_out = cnn_encoder_layer(cnn1_out, filter2, strides2)\n","\n","    # the third layer, the output size is 8 * 8 * 128\n","    filter3 = tensor_variable([2, 2, 64, 128], \"filter3\")\n","    strides3 = (1, 2, 2, 1)\n","    cnn3_out = cnn_encoder_layer(cnn2_out, filter3, strides3)\n","\n","    # the fourth layer, the output size is 4 * 4 * 256\n","    filter4 = tensor_variable([2, 2, 128, 256], \"filter4\")\n","    strides4 = (1, 2, 2, 1)\n","    cnn4_out = cnn_encoder_layer(cnn3_out, filter4, strides4)\n","\n","    return cnn1_out, cnn2_out, cnn3_out, cnn4_out\n","\n","\n","def cnn_lstm_attention_layer(input_data, layer_number):\n","    \"\"\"\n","    :param input_data:\n","    :param layer_number:\n","    :return:\n","    \"\"\"\n","    convlstm_layer = tf.contrib.rnn.ConvLSTMCell(\n","        conv_ndims=2,\n","        input_shape=[input_data.shape[2], input_data.shape[3], input_data.shape[4]],\n","        output_channels=input_data.shape[-1],\n","        kernel_shape=[2, 2],\n","        use_bias=True,\n","        skip_connection=False,\n","        forget_bias=1.0,\n","        initializers=None,\n","        name=\"conv_lstm_cell\" + str(layer_number))\n","\n","    outputs, state = tf.nn.dynamic_rnn(convlstm_layer, input_data, dtype=input_data.dtype)\n","\n","    # attention based on inner-product between feature representation of last step and other steps\n","    attention_w = []\n","    for k in range(step_max):\n","        attention_w.append(tf.reduce_sum(tf.multiply(outputs[0][k], outputs[0][-1])) / step_max)\n","    attention_w = tf.reshape(tf.nn.softmax(tf.stack(attention_w)), [1,step_max])\n","\n","    outputs = tf.reshape(outputs[0], [step_max, -1])\n","    outputs = tf.matmul(attention_w, outputs)\n","    outputs = tf.reshape(outputs, [1, input_data.shape[2], input_data.shape[3], input_data.shape[4]])\n","\n","    return outputs, attention_w\n"],"metadata":{"id":"UAzHX-XaeZJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_decoder_layer(conv_lstm_out_c, filter, output_shape, strides):\n","    \"\"\"\n","    :param conv_lstm_out_c:\n","    :param filter:\n","    :param output_shape:\n","    :param strides:\n","    :return:\n","    \"\"\"\n","\n","    deconv = tf.nn.conv2d_transpose(\n","        value=conv_lstm_out_c,\n","        filter=filter,\n","        output_shape=output_shape,\n","        strides=strides,\n","        padding=\"SAME\")\n","    deconv = tf.nn.selu(deconv)\n","    return deconv\n","\n","\n","def cnn_decoder(lstm1_out, lstm2_out, lstm3_out, lstm4_out):\n","    d_filter4 = tensor_variable([2, 2, 128, 256], \"d_filter4\")\n","    dec4 = cnn_decoder_layer(lstm4_out, d_filter4, [1, 8, 8, 128], (1, 2, 2, 1))\n","    dec4_concat = tf.concat([dec4, lstm3_out], axis=3)\n","\n","    d_filter3 = tensor_variable([2, 2, 64, 256], \"d_filter3\")\n","    dec3 = cnn_decoder_layer(dec4_concat, d_filter3, [1, 15, 15, 64], (1, 2, 2, 1))\n","    dec3_concat = tf.concat([dec3, lstm2_out], axis=3)\n","\n","    d_filter2 = tensor_variable([3, 3, 32, 128], \"d_filter2\")\n","    dec2 = cnn_decoder_layer(dec3_concat, d_filter2, [1, 30, 30, 32], (1, 2, 2, 1))\n","    dec2_concat = tf.concat([dec2, lstm1_out], axis=3)\n","\n","    d_filter1 = tensor_variable([3, 3, 3, 64], \"d_filter1\")\n","    dec1 = cnn_decoder_layer(dec2_concat, d_filter1, [1, 30, 30, 3], (1, 1, 1, 1))\n","\n","    return dec1\n"],"metadata":{"id":"fE0RQM8Pekpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_iters = 5\n","save_model_step = 1\n","\n","learning_rate = 0.0002\n","\n","threhold = 0.005\n","alpha = 1.5"],"metadata":{"id":"W7LN91tPfl2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_path = \"/content/drive/MyDrive/Predictive MAINTENANCE/\"\n","test_data_path = \"/content/drive/MyDrive/Predictive MAINTENANCE/\"\n","reconstructed_data_path = \"/content/drive/MyDrive/Predictive MAINTENANCE/\"\n"],"metadata":{"id":"K8z5XLE0fyWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    # Read dataset from file\n","    matrix_data_path = train_data_path + \"datatrain.npy\"\n","    matrix_gt_1 = np.load(matrix_data_path)\n","\n","    sess = tf.compat.v1.Session()\n","    \n","    tf.compat.v1.disable_eager_execution()\n","    data_input = tf.compat.v1.placeholder(tf.float32, [step_max, 30, 30, 3])\n","\n","    # cnn encoder\n","    conv1_out, conv2_out, conv3_out, conv4_out = cnn_encoder(data_input)\n","\n","    conv1_out = tf.reshape(conv1_out, [-1, 5, 30, 30, 32])\n","    conv2_out = tf.reshape(conv2_out, [-1, 5, 15, 15, 64])\n","    conv3_out = tf.reshape(conv3_out, [-1, 5, 8, 8, 128])\n","    conv4_out = tf.reshape(conv4_out, [-1, 5, 4, 4, 256])\n","\n","    # lstm with attention\n","    conv1_lstm_attention_out, atten_weight_1 = cnn_lstm_attention_layer(conv1_out, 1)\n","    conv2_lstm_attention_out, atten_weight_2 = cnn_lstm_attention_layer(conv2_out, 2)\n","    conv3_lstm_attention_out, atten_weight_3 = cnn_lstm_attention_layer(conv3_out, 3)\n","    conv4_lstm_attention_out, atten_weight_4 = cnn_lstm_attention_layer(conv4_out, 4)\n","\n","    # cnn decoder\n","    deconv_out = cnn_decoder(conv1_lstm_attention_out, conv2_lstm_attention_out, conv3_lstm_attention_out,\n","                             conv4_lstm_attention_out)\n","    # loss function: reconstruction error of last step matrix\n","    loss = tf.reduce_mean(tf.square(data_input[-1] - deconv_out))\n","    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n","\n","    # variable initialization\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","\n","    # training\n","    for idx in range(train_start_id, train_end_id):\n","        matrix_gt = matrix_gt_1[idx - train_start_id]\n","        feed_dict = {data_input: np.asarray(matrix_gt)}\n","        a, loss_value = sess.run([optimizer, loss], feed_dict)\n","        print(\"mse of last train data: \" + str(loss_value))\n","\n","    # test\n","    # Read the data from test file.\n","    matrix_data_path =test_data_path + \"datatest.npy\"\n","    matrix_gt_1 = np.load(matrix_data_path)\n","    result_all = []\n","    for idx in range(test_start_id, test_end_id):\n","        matrix_gt = matrix_gt_1[idx - test_start_id]\n","        feed_dict = {data_input: np.asarray(matrix_gt)}\n","        result, loss_value = sess.run([deconv_out, loss], feed_dict)\n","        result_all.append(result)\n","        print(\"mse of last test data: \" + str(loss_value))\n","\n","    # Write the reconstructed matrix to the file\n","    reconstructed_path =reconstructed_data_path\n","    if not os.path.exists(reconstructed_path):\n","        os.makedirs(reconstructed_path)\n","    reconstructed_path = reconstructed_path + \"test_reconstructed.npy\"\n","\n","    result_all = np.asarray(result_all).reshape((-1, 30, 30, 3))\n","    print(result_all.shape)\n","    np.save(reconstructed_path, result_all)\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"xu1qqeHUeytS","executionInfo":{"status":"error","timestamp":1648488413553,"user_tz":-330,"elapsed":520,"user":{"displayName":"Krishnendu Janardhanan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_C9QXaTiEaSjGRgoNAHTWeK72GIjKCJbf_Nrf=s64","userId":"02942996957088729370"}},"outputId":"4b0efb05-4e3e-459e-bf1d-d1f103cea332"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-eb8ab6ec9fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-eb8ab6ec9fba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# cnn encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mconv1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mconv1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-f455f54d758c>\u001b[0m in \u001b[0;36mcnn_encoder\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \"\"\"\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# the first layer,the output size is 30 * 30 * 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mfilter1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filter1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mstrides1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcnn1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_encoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-f455f54d758c>\u001b[0m in \u001b[0;36mtensor_variable\u001b[0;34m(shape, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"6mYOTog-jsBu"},"execution_count":null,"outputs":[]}]}